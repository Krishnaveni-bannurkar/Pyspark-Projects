{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b2c0fa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Json file Data\n",
      "+-----+-------------+-----------+---+------+-------------+----------+-----+-------+---------+\n",
      "|   id|     Gym_name|Person_name|age|  type|       street| City_name|state|country|unique_id|\n",
      "+-----+-------------+-----------+---+------+-------------+----------+-----+-------+---------+\n",
      "|12345|Fitness World| Jane Smith| 25| home1| 111 Pine St.|   Anytown|   CA|    USA|        1|\n",
      "|12345|Fitness World| Jane Smith| 25| home2|222 Maple St.|Otherville|   CA|    USA|        1|\n",
      "|12345|Fitness World| Jane Smith| 25|office|  333 Oak St.| Cityville|   CA|    USA|        1|\n",
      "|12345|Fitness World| John singh| 30| home1| 123 Main St.|   Anytown|   CA|    USA|        2|\n",
      "|12345|Fitness World| John singh| 30| home2|  456 Elm St.|Otherville|   CA|    USA|        2|\n",
      "|12345|Fitness World| John singh| 30|office|  789 Oak St.| Cityville|   CA|    USA|        2|\n",
      "|12345|Fitness World|Kiddo singh| 11| home1| 111 Pine St.|   Anytown|   CA|    USA|        3|\n",
      "|12345|Fitness World|Kiddo singh| 11| home2|222 Maple St.|Otherville|   CA|    USA|        3|\n",
      "|12345|Fitness World|Kiddo singh| 11|office|  333 Oak St.| Cityville|   CA|    USA|        3|\n",
      "|12345|Fitness World|kiddo Smith|  9| home1| 111 Pine St.|   Anytown|   CA|    USA|        4|\n",
      "|12345|Fitness World|kiddo Smith|  9| home2|222 Maple St.|Otherville|   CA|    USA|        4|\n",
      "|12345|Fitness World|kiddo Smith|  9|office|  333 Oak St.| Cityville|   CA|    USA|        4|\n",
      "+-----+-------------+-----------+---+------+-------------+----------+-----+-------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"The program reads in a JSON file containing gym data using the PySpark library. The JSON file contains nested structures, so the program uses the explode function to flatten the data and select the relevant columns. The program then creates a unique ID for each person based on their name and sorts the data based on this ID.\n",
    "\n",
    "Next, the program filters the data based on age eligibility (18 years and older) and creates two DataFrames: one for eligible people and one for ineligible people. Finally, the program writes the two DataFrames to CSV files.\n",
    "\n",
    "Overall, the program is an example of how to use PySpark to read in, manipulate, and analyze data stored in nested JSON structures.\n",
    "\"\"\"\n",
    "# Import necessary libraries\n",
    "from pyspark.sql import SparkSession, HiveContext\n",
    "from pyspark.sql.functions import explode, dense_rank, when\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder.appName(\"Gym Data\").getOrCreate()\n",
    "\"\"\"\n",
    "schema = StructType([\n",
    "    StructField(\"Gyms\", ArrayType(\n",
    "        StructType([\n",
    "            StructField(\"id\", LongType(), True),\n",
    "            StructField(\"Gym_name\", StringType(), True),\n",
    "            StructField(\"people\", ArrayType(\n",
    "                StructType([\n",
    "                    StructField(\"Person_name\", StringType(), True),\n",
    "                    StructField(\"age\", LongType(), True),\n",
    "                    StructField(\"addresses\", ArrayType(\n",
    "                        StructType([\n",
    "                            StructField(\"type\", StringType(), True),\n",
    "                            StructField(\"Street\", StringType(), True),\n",
    "                            StructField(\"city\", ArrayType(\n",
    "                                StructType([\n",
    "                                StructField(\"City_name\", StringType(), True),\n",
    "                                StructField(\"state\", LongType(), True),\n",
    "                                StructField(\"country\", LongType(), True),\n",
    "                                ])\n",
    "                        ), True)\n",
    "                        ])\n",
    "                    ), True)\n",
    "                ])\n",
    "            ), True)\n",
    "        ])\n",
    "    ), True)\n",
    "])\n",
    "json_data_df = spark.read.option('multiline', 'True').schema(schema).json('/Users/krishnaveni/Desktop/gym.json')\n",
    "\"\"\"\n",
    "# Read in the JSON data\n",
    "json_data_df = spark.read.option('multiline', 'True').json('/Users/krishnaveni/Desktop/n.json')\n",
    "\n",
    "# Explode the nested structures and select relevant columns\n",
    "json_data_df = json_data_df.select(explode(\"gyms\").alias(\"gym\"))\n",
    "json_data_df = json_data_df.select(\"gym.id\", \"gym.Gym_name\", explode(\"Gym.people\").alias(\"person\"))\n",
    "json_data_df = json_data_df.select(\"id\", \"Gym_name\", \"person.Person_name\", \"person.age\", explode(\"person.addresses\").alias(\"address\"))\n",
    "json_data_df = json_data_df.select(\"id\", \"Gym_name\", \"Person_name\", \"age\", \"address.type\", \"address.street\", \"address.city.City_name\", \"address.city.state\", \"address.city.country\")\n",
    "\n",
    "# Create a window partitioned by Gym_name and ordered by Person_name\n",
    "window = Window.partitionBy('Gym_name').orderBy('Person_name')\n",
    "\n",
    "# Add a unique ID column based on the window and assign to a new DataFrame\n",
    "df = json_data_df.withColumn('unique_id', dense_rank().over(window))\n",
    "\n",
    "# Print the DataFrame\n",
    "print(\"Json file Data\")\n",
    "df.show()\n",
    "\n",
    "# Select distinct names and ages based on the unique ID column and sort by it\n",
    "distinct_names = df.select('unique_id','Person_name','age').distinct()\n",
    "Ordered_df = distinct_names.orderBy(distinct_names['unique_id'].asc())\n",
    "\n",
    "# Create two DataFrames based on age eligibility\n",
    "result_df = Ordered_df.withColumn('is_eligible', when(Ordered_df['age'] > 18, 'Y').otherwise('N'))\n",
    "eligible_df = result_df.filter(result_df.is_eligible == 'Y')\n",
    "not_eligible_df = result_df.filter(result_df.is_eligible == 'N')\n",
    "\n",
    "# Write the DataFrames to CSV files\n",
    "eligible_df.write.csv(\"/Users/krishnaveni/Desktop/eligible1.csv\", header=True)\n",
    "not_eligible_df.write.csv(\"/Users/krishnaveni/Desktop/noteligible1.csv\", header=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d8e6f6a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
